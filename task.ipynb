{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrej/.virtualenvs/summer/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/andrej/.virtualenvs/summer/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# The package comes built-in with Python\n",
    "import re\n",
    "# http://www.nltk.org/install.html\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "# The package comes built-in with Python\n",
    "import re\n",
    "# http://www.nltk.org/install.html\n",
    "import nltk \n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pwd = \"/Users/andrej/Documents/Mine/Zemanta/Zemanta-Data-Science-Summer-School\"\n",
    "os.chdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-processing step\n",
    "def text_to_words(raw_text):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_text).get_text() \n",
    "\n",
    "    # 2. Remove non-letters\n",
    "    # Find anything that is NOT a lowercase letter (a-z) or an upper case letter (A-Z), and replace it with a space\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # There are many other things we could do to the data\n",
    "    # For example, Porter Stemming and Lemmatizing (both available in NLTK) \n",
    "    # would allow us to treat \"messages\", \"message\", and \"messaging\" as the same word\n",
    "    # which could certainly be useful.\n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    clear_text = \" \".join(meaningful_words)\n",
    "    return clear_text\n",
    "    #validate_text_sentiment[\"Message\"][i] = clear_text \n",
    "    #print i\n",
    "    #return( \" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrej/.virtualenvs/summer/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/task_train.csv\")\n",
    "df_train['cleaned_text'] = df_train.apply (lambda row: text_to_words(row.review), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, maxFeatures=3000, ngram=2):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, ngram), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    analyze = bigram_vectorizer.build_analyzer()\n",
    "    vectorizer = CountVectorizer(analyzer = analyze, tokenizer = None, preprocessor = None, stop_words = None, \n",
    "                                 max_features = maxFeatures)\n",
    "    \n",
    "    df_train1, df_validate1 = train_test_split(df_train, test_size=0.2)\n",
    "    train_data_features = vectorizer.fit_transform(df_train1['cleaned_text'].values.astype('U'))\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    \n",
    "    model.fit(train_data_features, df_train1['sentiment'].values)\n",
    "    \n",
    "    test_data_features = vectorizer.transform(df_validate1['cleaned_text'].values.astype('U'))\n",
    "    test_data_features = test_data_features.toarray()\n",
    "    \n",
    "    target_names = ['0', '1']\n",
    "    print(classification_report(df_validate1['sentiment'], model.predict(test_data_features), target_names=target_names))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.86      0.85       934\n",
      "          1       0.87      0.86      0.87      1032\n",
      "\n",
      "avg / total       0.86      0.86      0.86      1966\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.86      0.85       985\n",
      "          1       0.86      0.84      0.85       981\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1966\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.85      0.87      1009\n",
      "          1       0.85      0.87      0.86       957\n",
      "\n",
      "avg / total       0.86      0.86      0.86      1966\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.86      0.87      1007\n",
      "          1       0.85      0.87      0.86       959\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "\n",
    "# svm.NuSVC()\n",
    "# model = svm.SVC(C = 1, gamma = 0.001, kernel='rbf') # ngram=1, maxFeatures = 7000: 0.86\n",
    "# RandomForestClassifier(n_estimators=100, max_features=10 )\n",
    "model=LogisticRegression()\n",
    "for i in range(4):\n",
    "    if (i < 2):\n",
    "        testModel(model, ngram=2, maxFeatures=15000)\n",
    "    else:\n",
    "        testModel(model, ngram=2, maxFeatures=9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, max_features=10 )\n",
    "# forest = GradientBoostingClassifier()\n",
    "\n",
    "# param_grid = {'n_estimators': [200], 'max_features': [25,26,27, 28, 29, 30]}\n",
    "param_grid = {'n_estimators': [100], 'max_features': range(10,50,10)}\n",
    "grid_clf = GridSearchCV(forest, param_grid, cv=3, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_clf.fit(train_data_features, df_train1['sentiment'].values)\n",
    "grid_clf.fit(train_data_features, df_train1['sentiment'].values)\n",
    "best_model = grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print grid_clf.best_score_\n",
    "# print grid_clf.best_params_\n",
    "# print grid_clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test F1 validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(df_validate1['cleaned_text'].values.astype('U'))\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.83      0.85       998\n",
      "          1       0.83      0.88      0.85       968\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['0', '1']\n",
    "print(classification_report(df_validate1['sentiment'], best_model.predict(test_data_features), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "vectorizer = CountVectorizer(analyzer = analyze, tokenizer = None, preprocessor = None, stop_words = None, \n",
    "                             max_features = 10000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(df_train['cleaned_text'].values.astype('U'))\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "model.fit(train_data_features, df_train['sentiment'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/task_test.csv\")\n",
    "\n",
    "df_test['cleaned_text'] = df_test.apply (lambda row: text_to_words(row.review), axis=1)\n",
    "test_data_features = vectorizer.transform(df_test['cleaned_text'].values.astype('U'))\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "df_test['predicted sentiment'] = model.predict(test_data_features)\n",
    "df_test[['id', 'predicted sentiment']].to_csv(\"results/task_test_predicted.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6811_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1119_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9011_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12106_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  predicted sentiment\n",
       "0    7166_2                    0\n",
       "1   6811_10                    1\n",
       "2    1119_1                    0\n",
       "3    9011_9                    1\n",
       "4  12106_10                    1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['id', 'predicted sentiment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {}\n",
    "grid_clf = GridSearchCV(LogisticRegression(), param_grid, cv=10, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf.fit(train_data_features, df_train1['sentiment'].values)\n",
    "best_model = grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print grid_clf.best_score_\n",
    "print grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(df_validate1['cleaned_text'].values.astype('U'))\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_validate1['sentiment'], grid_clf.predict(test_data_features), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, max_features=10 )\n",
    "# forest = GradientBoostingClassifier()\n",
    "\n",
    "# param_grid = {'n_estimators': [200], 'max_features': [25,26,27, 28, 29, 30]}\n",
    "param_grid = {'n_estimators': [100], 'max_features': range(10,50,10)}\n",
    "grid_clf = GridSearchCV(forest, param_grid, cv=3, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_clf.fit(train_data_features, df_train1['sentiment'].values)\n",
    "grid_clf.fit(train_data_features, df_train1['sentiment'].values)\n",
    "best_model = grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836435000982\n",
      "{'max_features': 10, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print grid_clf.best_score_\n",
    "print grid_clf.best_params_\n",
    "# print grid_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test f1 validity\n",
    "test_data_features = vectorizer.transform(df_validate1['cleaned_text'].values.astype('U'))\n",
    "test_data_features = test_data_features.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.82      0.85      1013\n",
      "          1       0.82      0.88      0.85       953\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['0', '1']\n",
    "print(classification_report(df_validate1['sentiment'], best_model.predict(test_data_features), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = df_validate.iloc[:2]\n",
    "# testDF['cleaned_text'] = ['This is a super good great best movie I ever saw', 'This is a very bad movie']\n",
    "test_validate_data_features = vectorizer.transform(testDF['cleaned_text'].values.astype('U'))\n",
    "test_validate_data_features = test_validate_data_features.toarray()\n",
    "\n",
    "test_y_pred = nb_model.predict(test_validate_data_features)\n",
    "\n",
    "test_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   15    15    15 ...  8295 12729 13673]\n"
     ]
    }
   ],
   "source": [
    "df_train1, df_validate1 = train_test_split(df_train, test_size=0.2)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "vectorizer = CountVectorizer(analyzer = analyze, tokenizer = None, preprocessor = None, stop_words = None, \n",
    "                             max_features = 10000)\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# transformer = TfidfTransformer(smooth_idf=False)\n",
    "train_data_features = vectorizer.fit_transform(df_train1['cleaned_text'].values.astype('U'))\n",
    "train_data_features = train_data_features.toarray()\n",
    "y_actual = df_train.sentiment.values\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "dist.sort()\n",
    "print dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
